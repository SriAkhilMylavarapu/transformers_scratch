{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3ce921-ed56-466c-a173-081c6fb26cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e734bac-4f75-4877-8ae8-e7f73338efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import swifter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sacrebleu.metrics import BLEU\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.simplefilter(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e4ba11-ea1b-42f2-85b7-ce93edd1d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, self.d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,self.d_model,2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i/self.d_model)))\n",
    "                if i+1 < self.d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000 ** (i/self.d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe) #dont use for training\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.Tensor(self.pe[:,:seq_len])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55bd99be-9e97-4527-be18-a4ae70a35bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_model = 512, num_heads = 8\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        #We use weights of size (d_model, d_model) to represent the weights of all heads, the first (d_model, d_k) would be the weights of 1st head and so on.\n",
    "        self.W_q = nn.Linear(d_model, d_model) #weights for Queries\n",
    "        self.W_k = nn.Linear(d_model, d_model) #weights for Keys\n",
    "        self.W_v = nn.Linear(d_model, d_model) #weights for Values\n",
    "        self.W_o = nn.Linear(d_model, d_model) #weights for Outputs\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None): #Q,K,V are in the shape [batch_size, num_heads, seq_length, d_k]\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  #dot product\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e4)  #masking so softmax assigns them 0 probability\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)  #softmax\n",
    "        output = torch.matmul(attn_probs, V) #weighted distribution of Values using softmax\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)  #splits into d_k length vectors for multihead\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model) #reverses what split_heads does to return a d_model vector output\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q)) #split_heads is used here to represent the multiple heads \n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask) #Gets attention\n",
    "        output = self.W_o(self.combine_heads(attn_output)) #Linear output layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4aa029-198f-4491-87d4-3875f98e4272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two linear transformations and a ReLU activation\n",
    "#d_ff = 2048\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d09679-7361-454c-83e7-88313a6922d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask) #get self attention\n",
    "        x = self.norm1(x + self.dropout(attn_output)) #residual add and norm\n",
    "        ff_output = self.feed_forward(x) #feed forward\n",
    "        x = self.norm2(x + self.dropout(ff_output)) #residual add and norm\n",
    "        return x #encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46bb28d1-3a2d-4a1f-b659-e1226a89a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask): #masks are used so the decoder layer can't look further than the words it has predicted\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask) #masked self attention\n",
    "        x = self.norm1(x + self.dropout(attn_output)) #residual add and norm\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask) #masked cross attention\n",
    "        x = self.norm2(x + self.dropout(attn_output)) #residual add and norm\n",
    "        ff_output = self.feed_forward(x) #feed forward\n",
    "        x = self.norm3(x + self.dropout(ff_output)) #residual add and norm\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62bd6a8c-93ea-4f82-9058-ed2bcca0aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model) #word embedding for source language\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model) #word embedding for target language\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length) #positional encoding\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) #encoder layers (6)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) #decoder layers (6)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size) #linear layer for decoding output\n",
    "        self.dropout = nn.Dropout(dropout) #dropout after positional encoding\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2) #creates a boolean mask of shape (batch_size, 1, 1, seq_length)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) #makes it ignore padding tokens\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device) #makes it not look ahead\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src))) #word and positional embedding\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded  #run through num_layers of encoders\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded  #run through num_layers of decoders\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output) #final linear layer\n",
    "        tgt_word_probs = torch.softmax(output, dim=-1)  #softmax\n",
    "        return tgt_word_probs #final decoded output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02f898e-af89-410d-8a19-dd38ac0f0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(file_path, lang, vocab_size=20000):\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    "    )\n",
    "    def batch_iterator():\n",
    "        batch_size = 1000\n",
    "        for chunk in pd.read_csv(file_path, chunksize=batch_size, on_bad_lines=\"skip\", encoding=\"utf-8\", lineterminator='\\n'):  # Read in chunks\n",
    "            chunk = chunk.dropna()\n",
    "            yield chunk[lang].tolist()\n",
    "    \n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794c49a9-3b9d-42df-8b76-894ec4113986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_tokenizers(filepath, base_path=\"tokenizers\"):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    src_path = os.path.join(base_path, \"src_tokenizer.json\")\n",
    "    tgt_path = os.path.join(base_path, \"tgt_tokenizer.json\")\n",
    "    \n",
    "    if os.path.exists(src_path):\n",
    "        print(f\"Loading existing source tokenizer from {src_path}\")\n",
    "        src_tokenizer = Tokenizer.from_file(src_path)\n",
    "    else:\n",
    "        print(f\"Creating new source tokenizer and saving to {src_path}\")\n",
    "        src_tokenizer = create_tokenizer(filepath, \"de\")\n",
    "        src_tokenizer.save(src_path)\n",
    "    \n",
    "    if os.path.exists(tgt_path):\n",
    "        print(f\"Loading existing target tokenizer from {tgt_path}\")\n",
    "        tgt_tokenizer = Tokenizer.from_file(tgt_path)\n",
    "    else:\n",
    "        print(f\"Creating new target tokenizer and saving to {tgt_path}\")\n",
    "        tgt_tokenizer = create_tokenizer(filepath, \"en\")\n",
    "        tgt_tokenizer.save(tgt_path)\n",
    "    \n",
    "    return src_tokenizer, tgt_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e59a0d24-e68d-47be-86de-2c62b65d04bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing source tokenizer from tokenizers\\src_tokenizer.json\n",
      "Loading existing target tokenizer from tokenizers\\tgt_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer, tgt_tokenizer = get_or_create_tokenizers(\"wmt14_translate_de-en_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "186839cb-021c-4848-a0b7-56398b47011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"wmt14_translate_de-en_train.csv\", on_bad_lines=\"skip\", encoding=\"utf-8\", lineterminator='\\n').dropna()\n",
    "test_data = pd.read_csv(\"wmt14_translate_de-en_test.csv\", on_bad_lines=\"skip\", encoding=\"utf-8\", lineterminator='\\n').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c371dea-25da-4307-9bcc-5011bdb12981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(tokens, tokenizer, max_length=100):\n",
    "    pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "    if len(tokens) < max_length:\n",
    "        tokens = tokens + [pad_id] * (max_length - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_length-1] + [tokenizer.token_to_id(\"[EOS]\")]\n",
    "    return tokens\n",
    "\n",
    "def tokenize_line(line, tokenizer):\n",
    "    tokens = tokenizer.encode(line).ids\n",
    "    \n",
    "    # Add BOS and EOS tokens\n",
    "    tokens = [tokenizer.token_to_id(\"[BOS]\")] + tokens + [tokenizer.token_to_id(\"[EOS]\")]\n",
    "    \n",
    "    # Pad sequences\n",
    "    tokens = pad_sequence(tokens, tokenizer)\n",
    "    \n",
    "    return torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54b118b7-b91a-462d-8b68-6373bb59472c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8655da82954829b687e3831a4d279b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40c8a5604a74c83a857e56a76430d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab54916f3444c609d87b5b456900c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4508785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\swifter\\swifter.py:311\u001b[0m, in \u001b[0;36mSeriesAccessor.apply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr_logging():\n\u001b[1;32m--> 311\u001b[0m     tmp_df \u001b[38;5;241m=\u001b[39m func(sample, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    312\u001b[0m     sample_df \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mapply(func, convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mtokenize_line\u001b[1;34m(line, tokenizer)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_line\u001b[39m(line, tokenizer):\n\u001b[1;32m---> 10\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(line)\u001b[38;5;241m.\u001b[39mids\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Add BOS and EOS tokens\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TextInputSequence must be str",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mswifter\u001b[38;5;241m.\u001b[39mapply(tokenize_line, tokenizer\u001b[38;5;241m=\u001b[39msrc_tokenizer)\n\u001b[0;32m      2\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mswifter\u001b[38;5;241m.\u001b[39mapply(tokenize_line, tokenizer\u001b[38;5;241m=\u001b[39mtgt_tokenizer)\n\u001b[1;32m----> 3\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mswifter\u001b[38;5;241m.\u001b[39mapply(tokenize_line, tokenizer\u001b[38;5;241m=\u001b[39msrc_tokenizer)\n\u001b[0;32m      4\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mswifter\u001b[38;5;241m.\u001b[39mapply(tokenize_line, tokenizer\u001b[38;5;241m=\u001b[39mtgt_tokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\swifter\\swifter.py:329\u001b[0m, in \u001b[0;36mSeriesAccessor.apply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_apply(func, convert_dtype, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# use pandas\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pandas_apply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj, func, convert_dtype, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\swifter\\swifter.py:235\u001b[0m, in \u001b[0;36mSeriesAccessor._pandas_apply\u001b[1;34m(self, df, func, convert_dtype, *args, **kwds)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_bar:\n\u001b[0;32m    234\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mpandas(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_bar_desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas Apply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mprogress_apply(func, convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mapply(func, convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\apply.py:1496\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard.<locals>.curried\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurried\u001b[39m(x):\n\u001b[1;32m-> 1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tqdm\\std.py:911\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tqdm\\notebook.py:262\u001b[0m, in \u001b[0;36mtqdm_notebook.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;66;03m# cannot catch KeyboardInterrupt when using manual tqdm\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;66;03m# as the interrupt will most likely happen on another statement\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tqdm\\std.py:1241\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmoothing \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;129;01mand\u001b[39;00m dn:\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;66;03m# EMA (not just overall average)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(lock_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_args)\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tqdm\\std.py:231\u001b[0m, in \u001b[0;36mEMA.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalls \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    232\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    x  : float\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m        New value to include in EMA.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data['de'] = test_data['de'].swifter.apply(tokenize_line, tokenizer=src_tokenizer)\n",
    "test_data['de'] = test_data['en'].swifter.apply(tokenize_line, tokenizer=tgt_tokenizer)\n",
    "train_data['de'] = train_data['de'].swifter.apply(tokenize_line, tokenizer=src_tokenizer)\n",
    "train_data['en'] = train_data['en'].swifter.apply(tokenize_line, tokenizer=tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb88252-efe0-49c8-8210-4529e84b6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.iloc[1]['de'])\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa148a4-6b43-4860-8a24-e0538bdf605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        \n",
    "        return item['de'], item['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a8650-4899-422e-8ab6-ef7e578bc009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens_to_sentences(token_ids, tokenizer):\n",
    "    \"\"\"Convert token IDs back to sentences.\"\"\"\n",
    "    sentences = []\n",
    "    for seq in token_ids:\n",
    "        # Remove padding, end tokens, etc\n",
    "        seq = seq[seq != 0]  # Remove padding\n",
    "        seq = seq[seq != 2]  # Remove EOS token if present\n",
    "        sentence = tokenizer.decode(seq.tolist())\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def calculate_bleu(predictions, targets, tokenizer):\n",
    "    \"\"\"Calculate BLEU score for a batch of predictions.\"\"\"\n",
    "    # Convert token ids to sentences\n",
    "    pred_sentences = decode_tokens_to_sentences(predictions, tokenizer)\n",
    "    target_sentences = decode_tokens_to_sentences(targets, tokenizer)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    bleu = BLEU()\n",
    "    score = bleu.corpus_score(pred_sentences, [target_sentences])\n",
    "    \n",
    "    return score.score  # Returns the BLEU score as a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565cddfd-4858-4c21-bf51-fddb1ed26132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, criterion, optimizer, num_epochs, device, \n",
    "                     patience=5, checkpoint_dir='checkpoints', warmup_steps=4000, max_lr=0.01):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    training_history = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Learning rate scheduler setup\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=warmup_steps,  # First restart cycle length\n",
    "        T_mult=2,  # Multiply cycle length by 2 after each restart\n",
    "        eta_min=1e-4  # Minimum learning rate\n",
    "    )\n",
    "    \n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir, is_best=False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }\n",
    "        # Save regular checkpoint\n",
    "        path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        \n",
    "        # Save best model separately\n",
    "        if is_best:\n",
    "            best_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"Saved best model with loss: {loss:.4f}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_stats = {\n",
    "            'total_loss': 0,\n",
    "            'num_correct_predictions': 0,\n",
    "            'total_predictions': 0,\n",
    "            'total_tokens': 0,\n",
    "            'batch_losses': [],\n",
    "            'bleu_scores': []\n",
    "        }\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            decoder_input = tgt[:, :-1] #shifted right\n",
    "            target = tgt[:, 1:] #shifted left\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                output = model(src, decoder_input)\n",
    "                output_flat = output.contiguous().view(-1, output.size(-1))\n",
    "                target_flat = target.contiguous().view(-1)\n",
    "                loss = criterion(output_flat, target_flat)\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            with torch.no_grad():\n",
    "                # Get predictions\n",
    "                predictions = output.argmax(dim=-1)\n",
    "\n",
    "                bleu_score = calculate_bleu(predictions.cpu(), target.cpu(), tgt_tokenizer)\n",
    "                \n",
    "                # Update statistics\n",
    "                epoch_stats['total_loss'] += loss.item()\n",
    "                epoch_stats['batch_losses'].append(loss.item())\n",
    "                epoch_stats['num_correct_predictions'] += num_correct\n",
    "                epoch_stats['total_tokens'] += num_tokens\n",
    "                epoch_stats['bleu_scores'].append(bleu_score)\n",
    "            \n",
    "            # Print batch progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                current_loss = loss.item()\n",
    "                avg_loss = epoch_stats['total_loss'] / (batch_idx + 1)\n",
    "                avg_bleu = np.mean(epoch_stats['bleu_scores'])\n",
    "                \n",
    "                print(f'Epoch: {epoch+1}/{num_epochs} | Batch: {batch_idx}/{len(train_loader)} | '\n",
    "                      f'Loss: {current_loss:.4f} | Avg Loss: {avg_loss:.4f} | '\n",
    "                      f'LR: {current_lr:.6f}')\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_epoch_loss = epoch_stats['total_loss'] / len(train_loader)\n",
    "        avg_epoch_bleu = np.mean(epoch_stats['bleu_scores'])\n",
    "        \n",
    "        # Store training history\n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'avg_loss': avg_epoch_loss,\n",
    "            'bleu': avg_epoch_bleu,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'\\nEpoch {epoch+1} Statistics:')\n",
    "        print(f'Average Loss: {avg_epoch_loss:.4f}')\n",
    "        print(f'BLEU Score: {avg_epoch_bleu:.2f}')\n",
    "        print(f'Learning Rate: {current_lr:.6f}')\n",
    "        \n",
    "        # Save checkpoint and check for early stopping\n",
    "        is_best = avg_epoch_loss < best_loss\n",
    "        if is_best:\n",
    "            best_loss = avg_epoch_loss\n",
    "            patience_counter = 0\n",
    "            save_checkpoint(model, optimizer, scheduler, scaler, epoch, \n",
    "                          avg_epoch_loss, checkpoint_dir, is_best=True)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        save_checkpoint(model, optimizer, scheduler, scaler, epoch, \n",
    "                      avg_epoch_loss, checkpoint_dir)\n",
    "    \n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20b469-e9cb-4b0b-808c-7c81ea15c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 20000\n",
    "tgt_vocab_size = 20000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_dataset = TranslationDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, pin_memory=True, num_workers=0)\n",
    "\n",
    "train_transformer(transformer, train_dataloader, criterion, optimizer, num_epochs, device, patience=2, checkpoint_dir='checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46475567-0848-4385-970a-de04806d4d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305c422-94c5-4b38-8b4e-76108f7c1115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8488fd-8c82-4d1a-9458-0600fa2f3594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
